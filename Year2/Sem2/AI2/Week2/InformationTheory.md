# Information Theory
An event contains more information if it is less probable/more surprising
If two independent events are measured separately, the total amount of info is the sum of the self informations of the individual events

### Self Information
Given a random variable X with probability mass function $P_X(x)$, the self information of meauring X as outcome x is defined as $$I_X(x)=log_b [P_X(x)]=log_b \frac{1}{P_X(x)}$$
where different bases of the logarithm b results in different units


